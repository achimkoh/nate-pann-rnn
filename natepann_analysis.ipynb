{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.dirname(\"./data/\")\n",
    "talks_data = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if '.json' in f]\n",
    "\n",
    "talks = []\n",
    "for filepath in talks_data:\n",
    "    with open(filepath, 'r') as f:\n",
    "        try:\n",
    "            talks.append(json.load(f))\n",
    "        except json.JSONDecodeError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169163, 96542)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(talks_data), len(talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('엽기&호러', 17501),\n",
       " ('엔터톡', 16834),\n",
       " ('웃기는 사진/동영상', 6733),\n",
       " ('결혼/시집/친정', 6290),\n",
       " ('10대 이야기', 5466),\n",
       " ('동물 사랑방', 5303),\n",
       " ('해석 남/여', 4269),\n",
       " ('여자들끼리만', 3604),\n",
       " ('20대 이야기', 2795),\n",
       " ('지금은 연애중', 2697),\n",
       " ('배꼽조심 유머', 2668),\n",
       " ('사는 얘기', 2600),\n",
       " ('사랑, 고백해도 될까?', 2304),\n",
       " ('헤어진 다음날', 2261),\n",
       " ('개념상실한사람들', 1768),\n",
       " ('남편 vs 아내', 1669),\n",
       " ('사랑과 이별', 1651),\n",
       " ('요리&레시피', 1481),\n",
       " ('회사생활', 1076),\n",
       " ('세상에이런일이', 1017),\n",
       " ('훈훈한 이야기', 947),\n",
       " ('나억울해요', 835),\n",
       " ('건강/다이어트', 822),\n",
       " ('남자들끼리만', 629),\n",
       " ('임신/출산/육아', 619),\n",
       " ('뷰티&스타일', 583),\n",
       " ('여행을 떠나요', 350),\n",
       " ('스타/스포츠', 267),\n",
       " ('30대 이야기', 222),\n",
       " ('TV톡', 213),\n",
       " ('싱글톡', 181),\n",
       " ('대한민국 이슈', 156),\n",
       " ('판춘문예', 135),\n",
       " ('군화와 고무신', 121),\n",
       " ('취업과 면접', 89),\n",
       " ('알바 경험담', 67),\n",
       " ('맞벌이 부부 이야기', 39),\n",
       " ('컬투쇼', 38),\n",
       " ('군대일기', 38),\n",
       " ('백수&백조 이야기', 33),\n",
       " ('믿음과신앙', 32),\n",
       " ('묻고 답하기', 31),\n",
       " ('남자들의 속깊은 이야기', 27),\n",
       " ('이슈토론', 13),\n",
       " ('패션왕코리아', 12),\n",
       " ('포토 스토리', 9),\n",
       " ('웹툰&카툰', 8),\n",
       " ('40대 이야기', 6),\n",
       " ('로맨스가 더 필요해', 6),\n",
       " ('코미디빅리그', 5),\n",
       " ('음담패설', 4),\n",
       " ('야구야구', 3),\n",
       " ('언스타일', 3),\n",
       " ('직장여성의 애환', 3),\n",
       " ('방송의 적', 2),\n",
       " ('50대 이야기', 2),\n",
       " ('채널보기', 1),\n",
       " ('EXO', 1),\n",
       " ('아이유', 1),\n",
       " ('인피니트', 1),\n",
       " ('비스트', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([talk['category'].split(';')[0] for talk in talks]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22892, 26554, 12742, 10898, 4715)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsc = [talk for talk in talks if talk['category'].split(';')[0] == '결혼/시집/친정']\n",
    "gsc_text = [talk['text'] for talk in gsc]\n",
    "ent_text = [talk['text'] for talk in talks if talk['category'].split(';')[0] == '엔터톡']\n",
    "yupgi_text = [talk['text'] for talk in talks if talk['category'].split(';')[0] == '엽기&호러']\n",
    "teens_text = [talk['text'] for talk in talks if talk['category'].split(';')[0] == '10대 이야기']\n",
    "twenties_text = [talk['text'] for talk in talks if talk['category'].split(';')[0] == '20대 이야기']\n",
    "\n",
    "len(yupgi_text), len(ent_text), len(gsc_text), len(teens_text), len(twenties_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "gsc_dump = flatten(gsc_text)\n",
    "ent_dump = flatten(ent_text)\n",
    "yupgi_dump = flatten(yupgi_text)\n",
    "teens_dump = flatten(teens_text)\n",
    "twenties_dump = flatten(twenties_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3074, 3262, 4369, 3001, 2250)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(gsc_dump)),len(set(ent_dump)),len(set(yupgi_dump)),len(set(teens_dump)),len(set(twenties_dump))\n",
    "\n",
    "# not sure if character-based approach, like https://github.com/karpathy/char-rnn,\n",
    "# would be a good idea because of very high dimension (100x that of english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# markov.py by Allison Parrish\n",
    "# http://www.decontextualize.com/teaching/rwet/n-grams-and-markov-chains/\n",
    "\n",
    "import random\n",
    "\n",
    "def build_model(tokens, n):\n",
    "\t\"Builds a Markov model from the list of tokens, using n-grams of length n.\"\n",
    "\tmodel = dict()\n",
    "\tif len(tokens) < n:\n",
    "\t\treturn model\n",
    "\tfor i in range(len(tokens) - n):\n",
    "\t\tgram = tuple(tokens[i:i+n])\n",
    "\t\tnext_token = tokens[i+n]\n",
    "\t\tif gram in model:\n",
    "\t\t\tmodel[gram].append(next_token)\n",
    "\t\telse:\n",
    "\t\t\tmodel[gram] = [next_token]\n",
    "\tfinal_gram = tuple(tokens[len(tokens)-n:])\n",
    "\tif final_gram in model:\n",
    "\t\tmodel[final_gram].append(None)\n",
    "\telse:\n",
    "\t\tmodel[final_gram] = [None]\n",
    "\treturn model\n",
    "\n",
    "def generate(model, n, seed=None, max_iterations=100):\n",
    "\t\"\"\"Generates a list of tokens from information in model, using n as the\n",
    "\t\tlength of n-grams in the model. Starts the generation with the n-gram\n",
    "\t\tgiven as seed. If more than max_iteration iterations are reached, the\n",
    "\t\tprocess is stopped. (This is to prevent infinite loops)\"\"\" \n",
    "\tif seed is None:\n",
    "\t\tseed = random.choice(list(model)) # python 3 doesn't support indexing of dict keys. change to list\n",
    "\toutput = list(seed)\n",
    "\tcurrent = tuple(seed)\n",
    "\tfor i in range(max_iterations):\n",
    "\t\tif current in model:\n",
    "\t\t\tpossible_next_tokens = model[current]\n",
    "\t\t\tnext_token = random.choice(possible_next_tokens)\n",
    "\t\t\tif next_token is None: break\n",
    "\t\t\toutput.append(next_token)\n",
    "\t\t\tcurrent = tuple(output[-n:])\n",
    "\t\telse:\n",
    "\t\t\tbreak\n",
    "\treturn output\n",
    "\n",
    "def merge_models(models):\n",
    "\t\"Merges two or more Markov models.\"\n",
    "\tmerged_model = dict()\n",
    "\tfor model in models:\n",
    "\t\tfor key, val in model.iteritems():\n",
    "\t\t\tif key in merged_model:\n",
    "\t\t\t\tmerged_model[key].extend(val)\n",
    "\t\t\telse:\n",
    "\t\t\t\tmerged_model[key] = val\n",
    "\treturn merged_model\n",
    "\n",
    "def generate_from_token_lists(token_lines, n, count=14, max_iterations=100):\n",
    "\t\"\"\"Generates text from a list of lists of tokens. This function is intended\n",
    "\t\tfor input text where each line forms a distinct unit (e.g., poetry), and\n",
    "\t\twhere the desired output is to recreate lines in that form. It does this\n",
    "\t\tby keeping track of the n-gram that comes at the beginning of each line,\n",
    "\t\tand then only generating lines that begin with one of these \"beginnings.\"\n",
    "\t\tIt also builds a separate Markov model for each line, and then merges\n",
    "\t\tthose models together, to ensure that lines end with n-grams statistically\n",
    "\t\tlikely to end lines in the original text.\"\"\" \n",
    "\tbeginnings = list()\n",
    "\tmodels = list()\n",
    "\tfor token_line in token_lines:\n",
    "\t\tbeginning = token_line[:n]\n",
    "\t\tbeginnings.append(beginning)\n",
    "\t\tline_model = build_model(token_line, n)\n",
    "\t\tmodels.append(line_model)\n",
    "\tcombined_model = merge_models(models)\n",
    "\tgenerated_list = list()\n",
    "\tfor i in range(count):\n",
    "\t\tgenerated_str = generate(combined_model, n, random.choice(beginnings),\n",
    "\t\t\t\tmax_iterations)\t\n",
    "\t\tgenerated_list.append(generated_str)\n",
    "\treturn generated_list\n",
    "\n",
    "def char_level_generate(lines, n, count=14, max_iterations=100):\n",
    "\t\"\"\"Generates Markov chain text from the given lines, using character-level\n",
    "\t\tn-grams of length n. Returns a list of count items.\"\"\"\n",
    "\ttoken_lines = [list(line) for line in lines]\n",
    "\tgenerated = generate_from_token_lists(token_lines, n, count, max_iterations)\n",
    "\treturn [''.join(item) for item in generated]\n",
    "\n",
    "def word_level_generate(lines, n, count=14, max_iterations=100):\n",
    "\t\"\"\"Generates Markov chain text from the given lines, using word-level\n",
    "\t\tn-grams of length n. Returns a list of count items.\"\"\"\n",
    "\ttoken_lines = [line.split() for line in lines]\n",
    "\tgenerated = generate_from_token_lists(token_lines, n, count, max_iterations)\n",
    "\treturn [' '.join(item) for item in generated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsc_4 = build_model(gsc_dump, 4)\n",
    "ent_4 = build_model(ent_dump, 4)\n",
    "yupgi_4 = build_model(yupgi_dump, 4)\n",
    "teens_4 = build_model(teens_dump, 4)\n",
    "twenties_4 = build_model(twenties_dump, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네이트판 명예의 전당에 올라간 글을 재료로 텍스트를 생성하는 프로젝트를 시작했습니다.\n",
      "우선 어떻게 되는지 보기 위해 유니코드 문자 단위로 n-gram 마코프 모델을 만들어 텍스트를 만들어봤습니다.\n",
      "코드는 앨리슨 패리시 선생님의 수업 예제를 긁어왔습니다.\n",
      "출처: http://www.decontextualize.com/teaching/rwet/n-grams-and-markov-chains/\n",
      "n 값은 4입니다.\n",
      "카테고리마다 말투나 소재가 되는 단어 등이 다른 것을 어렴풋이 확인할 수 있습니다. 아직 제대로 된 문장이 나오지는 않습니다.\n",
      "\n",
      "=====결혼/시집/친정=====\n",
      "걸로 민감해지네요..시어머님 도와드리는데, 그러다 아이한테 점까지도 제사 어머님께 화낸적도 없는 빈공간있는데, 저는 하루도안빼고 마사지도 3달이 지났는데 그래도 아들얘기하지만 누가봐도 인정을 못받았어요 어머니가 아니었다고 함. 남자친구는 \"어맛! 왜 저의 사정은 자주쓰는데 두시간쯤떨어진곳에 있었습니다.\n",
      "아내의입장에선 말그대로 친정따로 가야하고, 실제로 남자친구있고요. 미안한 부분도 있었고.\n",
      " \n",
      "이렇게 오랫동안 안가다가 할아버지 앞에 계시던 다른방에서 여자는 제가 가고, 스님처럼 쿨하게 넘어가면 옆으로 누\n",
      "\n",
      "========엔터톡========\n",
      "괜히 곤란해요ㅠㅠㅜㅜㅜㅜㅜㅜㅜㅜㅜ 또 노래를 조카 잘생기고 멋지네\n",
      "의외의 창섭이는 자세도 하나도 없었고\n",
      "미모도 상큼하면서 본 지평선은 반듯반듯한 이정재야...응... 우린 그럴수 있죠 \n",
      "아파하는줄알아... 이름 겹치는 바람 맞는 거 보기 좋은 것 같다......!!\n",
      "제가 알아서 \n",
      "공중부양하면서 \n",
      "시크한모습 너무 안타까웠던 루나는 뮤지션은 랩퍼들은 뺌\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "http://fimg2.pann.com/new/download.jsp?FileID=33324]\n",
      "http://fimg2.pann.co\n",
      "\n",
      "=======엽기&호러=======\n",
      "를\n",
      "휘감고 사는 남자를 꺼낸 적이 있어서 뒤를 돌아보았으나 조용한 기분이 나빠졌는지 어떤 여자가 갑자기 목걸이에서 죽었다고 함.\n",
      "그 밖의 일. 조금 넘고, 늦게 간다고 절대!\n",
      "양치질을 마음대로 뽑을려고 그렇게 다시 들리는거야,\n",
      " \n",
      " \n",
      "곧 반란을 일으킨 자세를 잡았을때 다행히도)\n",
      "그렇게 넘기자 하고 부랴부랴 준비하시던 말씀이 \"애비야.. \n",
      "  \n",
      "상진은 자신이 너무나도 나에겐 도움이 되었는데\n",
      " \n",
      "내가 아닌 커피숖을 때려주고 우결처럼 부지런히 정리 : 5,500kg의 초대장을 맞이한 스포트라이트 일본자금으로 \n",
      "\n",
      "======10대 이야기======\n",
      "! 외국분이 동영상 캡쳐해놓는애 심리가 뭐 어떻냐면\n",
      "아 뭐 비리나고 그래서요즘엔거의이런말투\n",
      "오늘 친구를 안 가지만 써보자면 여기서 열폭하면 뭐든 해결할수 있는데 이젠 멀쩡한걸 왜 쳐마시는 사람이되는거였는데 두줄나왓데ㅠㅠㅠㅠ이건 진짜 무거워...오늘 다 풀어헤치며 띠꺼운 표정으로 했는데 거기선 나 진짜 여전히 속쌍임...그출입구가 되겠습니다! 그리고 나왔으면.....\n",
      "이렇게 살건가싶은 생각을 했거든...그냥 앞머리를 꼬드김 \n",
      "[img http://fimg3.pann.com/new/download.jsp?F\n",
      "\n",
      "======20대 이야기======\n",
      "에 한참 흥할때부터 고3까지도 우리나라 민심이 정말 대학생인데 막 말하는걸까?\n",
      "카톡을  계속 연락하는건가?^^ 참...ㅋㅋㅋㅋㅋ저보다 7살이나 많으시고 부디 안좋냐고..\n",
      "방금 만나러 갈거면 억지로 참고 운동시작해서 그렇게 친구사귀면서 항상 관계를했는데 다들 행복하게 사는여자중한명인데......... ㅋㅋ이런거 진짜 그냥 클럽처럼 꾸밀생각인데 어떻게 변하면 연애할때 꼭 빠질 수 없네용 ㅠㅠ 게다가 골반도 넓구\n",
      "암튼 되게 소심한 여자라서 아무런 연락이오길시작해요 ㅠ 그놈의 본질에만 집중했나벼 20살까지만 해\n",
      "\n",
      "=====================\n",
      "\n",
      "다음 단계로는 자료를 신경망에 집어넣을 경우 더 그럴싸한 문장을 생성할 수 있는지 실험해보려 합니다.\n",
      "안드레이 카파시의 유명한 프로젝트를 참고할 예정입니다: https://github.com/karpathy/char-rnn\n",
      "다만 영문 알파벳보다 한국어의 feature dimension이 훨씬 클 테므로 (완성형 2350자를 전제하더라도 영문의 수십 배)\n",
      "어떻게 더 효과적인 학습이 가능할지는 더 알아봐야 할 듯합니다.\n",
      "김태훈 님의 한국어 시 생성 프로젝트도 좀 더 자세히 보려고 합니다: https://github.com/carpedm20/poet-neural\n",
      "또한 그냥 문자열을 쓰지 않고 konlpy 패키지 등을 이용해 형태소로 변환한 자료를 사용하면 어떨지도 봐야겠습니다.\n"
     ]
    }
   ],
   "source": [
    "# gsc_gen = generate(gsc_4, 4, max_iterations=280)\n",
    "# ent_gen = generate(ent_4, 4, max_iterations=280)\n",
    "# yupgi_gen = generate(yupgi_4, 4, max_iterations=280)\n",
    "# teens_gen = generate(teens_4, 4, max_iterations=280)\n",
    "# twenties_gen = generate(twenties_4, 4, max_iterations=280)\n",
    "\n",
    "print('네이트판 명예의 전당에 올라간 글을 재료로 텍스트를 생성하는 프로젝트를 시작했습니다.')\n",
    "print('우선 어떻게 되는지 보기 위해 유니코드 문자 단위로 n-gram 마코프 모델을 만들어 텍스트를 만들어봤습니다.')\n",
    "print('코드는 앨리슨 패리시 선생님의 수업 예제를 긁어왔습니다.')\n",
    "print('출처: http://www.decontextualize.com/teaching/rwet/n-grams-and-markov-chains/')\n",
    "print('n 값은 4입니다.')\n",
    "print('카테고리마다 말투나 소재가 되는 단어 등이 다른 것을 어렴풋이 확인할 수 있습니다. 아직 제대로 된 문장이 나오지는 않습니다.')\n",
    "print('\\n=====결혼/시집/친정=====')\n",
    "print(''.join(gsc_gen))\n",
    "print('\\n========엔터톡========')\n",
    "print(''.join(ent_gen))\n",
    "print('\\n=======엽기&호러=======')\n",
    "print(''.join(yupgi_gen))\n",
    "print('\\n======10대 이야기======')\n",
    "print(''.join(teens_gen))\n",
    "print('\\n======20대 이야기======')\n",
    "print(''.join(twenties_gen))\n",
    "print('\\n=====================')\n",
    "print('')\n",
    "print('다음 단계로는 자료를 신경망에 집어넣을 경우 더 그럴싸한 문장을 생성할 수 있는지 실험해보려 합니다.')\n",
    "print('안드레이 카파시의 유명한 프로젝트를 참고할 예정입니다: https://github.com/karpathy/char-rnn')\n",
    "print('다만 영문 알파벳보다 한국어의 feature dimension이 훨씬 클 테므로')\n",
    "print('어떻게 효과적인 학습이 가능할지는 더 알아봐야 할 듯합니다.')\n",
    "print('김태훈 님의 한국어 시 생성 프로젝트도 좀 더 자세히 보려고 합니다: https://github.com/carpedm20/poet-neural')\n",
    "print('또한 그냥 문자열을 쓰지 않고 konlpy 패키지 등을 이용해 형태소로 변환한 자료를 사용하면 어떨지도 봐야겠습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "# from konlpy.tag import Hannanum, Kkma, Komoran, Mecab\n",
    "from konlpy.utils import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hannanum = Hannanum() # divides up morphemes too much\n",
    "# kkma = Kkma() # slow\n",
    "# komoran = Komoran() # memory error \n",
    "# mecab = Mecab() # install fail\n",
    "twitter = Twitter() # seems to be currently best for text generation\n",
    "pprint(kkma.sentences(gsc[0]['text']))\n",
    "print(' '.join(twitter.morphs(gsc_text[0])))\n",
    "# morphemes ignore whitespace. can I featurize whitespace with konlpy?\n",
    "# or, can the morphemes be combined back to sentences with whitespace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4169627, 57122)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# twitter_morphs = []\n",
    "# for text in gsc_text:\n",
    "#     twitter_morphs += twitter.morphs(text)\n",
    "# len(twitter_morphs), len(set(twitter_morphs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
